[[modules-node]]
== 节点

每启动一个 Elasticsearch 实例，就是启动了一个 _节点_。互相连接的节点的集合称之为<<modules-cluster,集群>>。若只运行了一个 Elasticsearch 节点，则是一个节点的集群。

集群中的每个节点默认都能处理 <<modules-http,HTTP>> 和 <<modules-transport,传输>> 流量。传输层只用于节点间和 {javaclient}/transport-client.html[Java `传输客户端`]间的通信；HTTP 蹭只用于外部 REST 客户端。

集群中的所有节点都知晓其他节点并能够向合适的节点发送客户端请求。除此之外，每个节点有一个或多个功能：

<<master-node,候选主节点>>::

`node.master` 设置为 `true`（默认）的节点，表示有资格<<modules-discovery-zen,被选为 _主_ 节点>>，也就是控制整体集群的节点。

<<data-node,数据节点>>::

 `node.data` 设置为 `true`（默认）的节点。数据节点保存数据以及进行与数据相关的操作比如 CRUD，搜索，和聚合。

<<ingest,摄入节点>>::

`node.ingest` 设置为 `true`（默认）的节点。摄入节点可以在每个文档被索引之前为其应用一个<<pipeline,摄入管道>>来对它进行变形和增加内容。当摄入负载高时，理应使用专门的摄入节点，同时设置主节点和数据节点为 `node.ingest: false`。

<<modules-tribe,部落节点>>::

部落节点，通过设置 `tribe.*` 配置, 是一个特殊的只用来协调的节点，它能连接到多个集群并且可以跨集群执行搜索和其他操作。

一个节点默认是候选主节点和数据节点，另外也能通过摄入管道对文档做预处理。这对小型集群是很方便的，但随着集群增大，考虑将专有的主节点和数据节点分开是很重要的。

[NOTE]
[[coordinating-node]]
.协调节点
===============================================

诸如搜索或批量索引的请求可能涉及不同的数据节点的数据。例如，一个搜索请求，就被分成两个阶段来执行。接收到客户端请求而协调这两个阶段的节点就是 _协调节点_。

在 _分发_ 阶段，协调节点将请求转发到持有相关数据的数据节点。每个数据节点都本地执行请求并将结果返回给协调节点。在 _收集_ 阶段，协调节点将每个数据节点的结果聚合成一个单一的全局的结果集。

每个节点都隐然地是协调节点。这意味着，设置 `node.master`，`node.data` 和 `node.ingest` 三个都为 `false` 的节点将只作为协调（无法禁用）节点。因此，协调节点需要拥有足够的内存和 CPU 来处理收集阶段。

===============================================

[float]
[[master-node]]
=== 候选主节点

主节点负责轻量级的集群范围的动作比如创建或删除一个索引，跟踪哪个节点属于集群，以及决定分片被分配到哪个节点。拥有一个稳定的主节点对集群健康很重要。

任意候选主节点（默认全部节点）都可以通过<<modules-discovery-zen,主选举流程>>被选举成为主节点。

IMPORTANT: 主节点必须拥有 `data/` 目录的权限（同数据节点），因为这是在节点重启中集群状态保存的地方。

索引和查询数据是 CPU，内存以及 I/O 密集型工作，会对节点的资源造成压力。为确保主节点稳定并且不产生过大压力，在较大的集群中将专用候选主节点和专用数据节点两种角色区分开来是一个好主意。

尽管主节点也能作为<<coordinating-node,协调节点>>路由从客户端来的查询以及索引请求到数据节点，建议最好不要让专用主节点来承担这个任务。为了集群稳定性，让候选主节点做尽量少的工作是很重要的。

按以下设置创建一个专用候选主节点：

[source,yaml]
-------------------
node.master: true <1>
node.data: false <2>
node.ingest: false <3>
cluster.remote.connect: false <4>
-------------------
<1> 默认启用 `node.master` 角色。
<2> 禁用 `node.data` 角色（默认启用）。
<3> 禁用 `node.ingest` 角色（默认启用）。
<4> 禁用跨集群查询（默认启用）。

ifdef::include-xpack[]
NOTE: 这些配置仅在为安装 {xpack} 时生效。当安装了 {xpack} 时，要创建专用候选主节点，请看 <<modules-node-xpack,{xpack} 节点配置>>。
endif::include-xpack[]


[float]
[[split-brain]]
==== 用 `minimum_master_nodes` 防止脑裂

为防止数据丢失，配置 `discovery.zen.minimum_master_nodes`（默认为 `1`）参数以使每个候选主节点知晓形成集群所需要的最少的可见候选主节点数目是非常关键的。

为了解释，假设有一个由两个候选主节点组成的集群。一个网络异常断开了这两节点间的通讯，现在每个节点看到一个候选主节点……它自身。如果 `minimum_master_nodes` 设置为默认值 `1`，这已足够形成一个集群。每个节点都将选举自己成为新的主节点（认为另一个候选主节点已经宕机），那么结果就是形成了两个集群，或称 _脑裂_。这两个节点将永不会重新连接直到其中一个重启。任何已经写入重启了的节点的数据都将丢失。

现在假设有一个由三个候选主节点组成的集群，并且 `minimum_master_nodes` 设置为 `2`。如果网络异常将一个节点与另外两个节点分隔，只有一个节点的这边发现集群中没有足够数量的候选主节点，所以它不会选举自己为主节点。有两个节点的那边将会选举一个新的主节点（需要的话）并且继续正常运行。一旦网络异常被解决，单一的节点将会重新加入集群并开始正常处理请求。

改配置应当设置为候选主节点的 _仲裁_：

  (master_eligible_nodes / 2) + 1

亦即，如果有三个候选主节点，最少的主节点数目应当设置为 `(3 / 2) + 1` 或 `2`：

[source,yaml]
----------------------------
discovery.zen.minimum_master_nodes: 2 <1>
----------------------------
<1> 默认为 `1`。

为了保证在一个候选主节点宕机时始终可用，集群应当至少有三个候选主节点，`minimum_master_nodes` 也必须相应的设置好。一次<<rolling-upgrades,滚动升级>>，在无宕机的情况下进行，也要求至少有三个候选主节点来避免升级进行时发生网络异常而导致的数据丢失的可能性。

该配置也能在一个活跃集群中通过<<cluster-update-settings,集群配置更新 API>> 进行动态配置：

[source,js]
----------------------------
PUT _cluster/settings
{
  "transient": {
    "discovery.zen.minimum_master_nodes": 2
  }
}
----------------------------
// CONSOLE
// TEST[catch:/cannot set discovery.zen.minimum_master_nodes to more than the current master nodes/]

TIP: 在专用节点间分离主节点和数据节点的一个优势是只需要三个候选主节点并且把 `minimum_master_nodes` 设置为 `2`。无论之后加入多少个专用数据节点，这个配置都无需变更。

[float]
[[data-node]]
=== 数据节点

数据节点保存分片，分片包含已经被索引的文档。数据节点处理与数据有关的操作比如 CRUD，查询和聚合。这些操作是 I/O，内存以及 CPU 密集型的，监控这些硬件资源以及在资源超负荷时增加数据节点是很重要的。

使用专用数据节点的最大好处就是讲主节点和数据节点的角色分离。

按以下设置创建一个专用数据节点：

[source,yaml]
-------------------
node.master: false <1>
node.data: true <2>
node.ingest: false <3>
cluster.remote.connect: false <4>
-------------------
<1> 禁用 `node.master` 角色（默认启用）。
<2> 默认启用 `node.data` 角色。
<3> 禁用 `node.ingest` 角色（默认启用）。
<4> 禁用跨集群搜索（默认启用）。

ifdef::include-xpack[]
NOTE: 这些配置仅在为安装 {xpack} 时生效。当安装了 {xpack} 时，要创建专用数据节点，请看 <<modules-node-xpack,{xpack} 节点配置>>。
endif::include-xpack[]

[float]
[[node-ingest-node]]
=== 摄入节点

摄入节点能执行有一个或多个摄入处理器组成的预处理管道。根据摄入处理器所执行的操作类型和所需资源，可能需要专用摄入节点来专门执行这个任务。

按以下设置创建一个专用摄入节点：

[source,yaml]
-------------------
node.master: false <1>
node.data: false <2>
node.ingest: true <3>
cluster.remote.connect: false <4>
-------------------
<1> 禁用 `node.master` 角色（默认启用）。
<2> 禁用 `node.data` 角色（默认启用）。
<3> 默认启用 `node.ingest` 角色。
<4> 禁用跨集群搜索（默认启用）。

ifdef::include-xpack[]
NOTE: 这些配置仅在为安装 {xpack} 时生效。当安装了 {xpack} 时，要创建专用摄入节点，请看 <<modules-node-xpack,{xpack} 节点配置>>。
endif::include-xpack[]

[float]
[[coordinating-only-node]]
=== 仅协调节点

如果拿掉处理主（节点）职责，保存数据和预处理文档的能力，那么就剩下一个仅协调节点，它只能路由请求，处理搜索的 reduce 阶段，以及分发批量索引。最关键地，仅协调节点可作为负载均衡器。

通过从数据和候选主节点中去负载和协调节点角色，仅协调节点可使大型集群受益。同其他节点一样，他们加入集群并接收全部的<<cluster-state,集群状态>>，然后利用集群状态直接路由请求到合适的地方。

WARNING: 向一个集群增加太多的仅协调节点会增加整个集群的负担，因为主节点必须等待全部节点更新集群状态！仅协调节点的好处不应该被高估——数据节点也能愉快地服务于相同的目的。

按以下设置创建专用协调节点：

[source,yaml]
-------------------
node.master: false <1>
node.data: false <2>
node.ingest: false <3>
cluster.remote.connect: false <4>
-------------------
<1> 禁用 `node.master` 角色（默认启用）。
<2> 禁用 `node.data` 角色（默认启用）。
<3> 禁用 `node.ingest` 角色（默认启用）。
<4> 禁用跨集群搜索（默认启用）。

ifdef::include-xpack[]
NOTE: 这些配置仅在为安装 {xpack} 时生效。当安装了 {xpack} 时，要创建专用仅协调节点，请看 <<modules-node-xpack,{xpack} 节点配置>>。
endif::include-xpack[]

[float]
== 节点数据路径配置

[float]
[[data-path]]
=== `path.data`

每个数据和主节点都必须拥有数据目录的访问权限，这个目录保存分片和索引还有集群元信息。`path.data` 默认为 `$ES_HOME/data`，但能在配置文件 `elasticsearch.yml` 中配置一个绝对路径或相对 `$ES_HOME` 的路径，如下所示：

[source,yaml]
-----------------------
path.data:  /var/elasticsearch/data
-----------------------

正如所有节点配置，也能在命令行中指定：

[source,sh]
-----------------------
./bin/elasticsearch -Epath.data=/var/elasticsearch/data
-----------------------

TIP: 当使用 `.zip` 或 `.tar.gz` 安装时，`path.data` 应该配置到 Elasticsearch 根目录之外的目录，这样删除根目录才不会删除数据！RPM 和 Debian 安装已经做好了这个配置。

[float]
[[max-local-storage-nodes]]
=== `node.max_local_storage_nodes`

<<data-path,数据路径>>能被多个节点共享，甚至是来自不同集群的节点。这对测试故障失效和开发机上的不同配置很有利。然而在生产环境，建议每个服务器只运行一个 Elasticsearch 节点。

Elasticsearch 默认配置为不让超过一个节点共享同一个数据路径。为了允许超过一个节点（例如，在开发机上），使用配置 `node.max_local_storage_nodes` 并将它设为一个大于1的整数。

WARNING: 切忌运行不同的节点类型（即是，主，数据）在同一个数据目录。这会导致意想不到的数据丢失。

[float]
== 其他节点配置

更多节点配置在<<modules,模块>>。特别值得注意的是<<cluster.name,`cluster.name`>>，<<node.name,`node.name`>>和<<modules-network,网络配置>>。

ifdef::include-xpack[]
include::ml-node.asciidoc[]
endif::include-xpack[]
