[[allocation-awareness]]
=== 分片分配感知

当在同一个物理服务器的多个VM、多个机架或跨多个区域或域上，运行多个节点时，2个不相关的节点不太会同时挂掉，但极有可能的是同一台物理服务器上，同个机架上，或者同个区域或者域中的2个节点同时崩溃。

如果 Elasticsearch 对你的硬件物理配置有所感知，它就能确保主分片和其副本分片分布在不同的物理服务器，机架，或者区域，以此最小化同时丢失全部分片副本的风险。

分片分配感知的设定允许你告诉 Elasticsearch 你的硬件配置信息。

举个例子，假设我们有几个机架，当我们启动一个节点时，我们可以通过给它分配一个名为 `rack_id` 的任意元数据属性来告诉它处于哪个机架中——我们可以使用任何属性名。例如：

[source,sh]
----------------------
./bin/elasticsearch -Enode.attr.rack_id=rack_one <1>
----------------------
<1> 该设定也能在 `elasticsearch.yml` 文件中进行指定。

现在，我们必须通过告诉 Elasticsearch 该使用哪些属性来配置 _分片分配感知_。 这可以在 *全部* 候选主节点的 `elasticsearch.yml` 文件中设置，或者通过<<cluster-update-settings,集群-更新-设定>> API 来设置（更改）。

比如，我们在配置文件中设这样的值：

[source,yaml]
--------------------------------------------------------
cluster.routing.allocation.awareness.attributes: rack_id
--------------------------------------------------------

有了上述配置，比方说我们启动两个节点包含配置 `node.attr.rack_id` 为 `rack_one`，然后建立包含5个主分片和1个副本的索引。所有主分片和副本在这两个节点间进行分配。
With this config in place, let's say we start two nodes with
`node.attr.rack_id` set to `rack_one`, and we create an index with 5 primary
shards and 1 replica of each primary.  All primaries and replicas are
allocated across the two nodes.

现在，如果我们启动另外两个节点包含配置 `node.attr.rack_id` 为 `rack_two`，Elasticsearch 将会移动分片到新的节点，确保（可能的话）同一个机架上不会存有同一个分片的两个拷贝。如果 `rack_two` 挂掉，其上节点也挂掉，Elasticsearch 仍会将丢失的分片拷贝分配给 `rack_one` 中的节点。

.首选本地分片
*********************************************

当执行搜索或 GET 请求时，若激活了分片感知，Elasticsearch 将会首选使用本地分片——在同一个感知组内的分片——来执行请求。这通常比跨越机架或者区域边界来得快。

*********************************************

可以指定多个感知属性，这种情形下，当决定分配分片到哪儿时将分别考虑每个属性。

[source,yaml]
-------------------------------------------------------------
cluster.routing.allocation.awareness.attributes: rack_id,zone
-------------------------------------------------------------

NOTE: 当使用感知属性时，分片不会被分配到不具有那些属性值的节点。

NOTE: 具有相同感知属性值的特定节点组上分配的分片的主/副本的数量由属性值的数量决定。当组中的节点数量不平衡并且存在许多副本时，副本分片可能处于未分配状态。

[float]
[[forced-awareness]]
=== 强制感知

想象你有两个区域且在这两区域有足够的的硬件来托管你全部的主副本分片。但也许单一区域的硬件，尽管足够托管一半的分片，也无法保存 *全部* 分片。

使用普通感知，若一个区域与另一个区域失去联系，Elasticsearch 将把所有的丢失的副本分片分配到单个区域。但在此例中，这个突然增加的负载会导致剩余区域超载。

强制感知通过 *绝不* 允许同一个分片的拷贝分配到同一个区域来解决这个问题。

例如，假设我们有一个称为 `zone` 的感知属性，我们知道我们将有两个区域，`zone1` 和 `zone2`。我们可以这样来配置一个节点上的强制感知：

[source,yaml]
-------------------------------------------------------------------
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 <1>
cluster.routing.allocation.awareness.attributes: zone
-------------------------------------------------------------------
<1> 我们必须列举 `zone` 属性的全部值。

现在，如果我们以 `node.attr.zone` 设置为 `zone1` 来启动2个节点，然后创建一个5个主分片1个副本的索引。索引将成功创建，但是只有主分片会被分配（副本不会）。只有当我们以 `node.attr.zone` 设置为 `zone2` 来启动更多的节点时副本才会开始分配。

`cluster.routing.allocation.awareness.*` 的设定可以在运行中的集群中通过<<cluster-update-settings,集群-更新-设定>> API 进行动态更新。
