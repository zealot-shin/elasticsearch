[[misc-cluster]]
=== 杂项集群设定

[[cluster-read-only]]
==== 元数据

整个集群可通过以下 _动态_ 设定被设定为只读：

`cluster.blocks.read_only`::

      使整个集群变为只读（索引不能进行写操作），元数据变为不可更改（创建或者删除索引）。

`cluster.blocks.read_only_allow_delete`::

      等同于 `cluster.blocks.read_only` 但允许删除索引来释放资源。

WARNING: 不要仰仗这些设定来防止对你集群的变更。任何用户只要能访问 <<cluster-update-settings,集群-更新-设定>> API 都能使集群再变回可读写状态。

[[cluster-shard-limit]]

==== 集群分片限制

在 Elasticsearch 7.0 及以后版本中，会有一个基于集群中节点数量的，对集群中分片数量的限制。这是为了防止某些操作无意中破坏集群的稳定。在7.0以前，那些可能导致集群超过这一限制的动作将发出过期警告。

NOTE: 可以将系统属性 `es.enforce_max_shards_per_node` 设置为 `true`，以选择严格执行碎片限制。如果设置了此系统属性，将导致集群超出限制的操作将导致错误，而不是过期警告。在 Elasticsearch 7.0中将删除此属性，因为严格执行限制是默认且仅有的行为。

如果一个操作，比如创建一个新索引，恢复索引的快照，或者打开封闭索引会导致集群中的碎片数量超过此限制，操作将发出过期警告。

如果集群已经超过限制，则由于节点成员关系的更改，或者设置更改时，创建或打开索引的所有操作都将发出警告，直到要么如下所述提高限制或者某些索引被<<indices-open-close,关闭>>或<<indices-delete-index,删除>>使分片数低于限制。

副本会计入这个极限，但是关闭的索引不会。一个5主分片和2个副本的索引将被计算为15个分片。任何关闭的索引不管它包含多少分片和副本，都被计为0。

这个限制默认为每节点1000个分片，可用以下属性动态调整：

`cluster.max_shards_per_node`::

     控制集群中每个节点所允许的分片个数。

例如，一个3节点集群使用默认设定一共可容许3000个分片，分布在所有打开的索引里。如果上述设定变为1500，那么集群一共可容许4500个分片。

[[user-defined-data]]
==== 用户自定义集群元数据

可以使用集群设定 API 存储和检索用户定义的元数据。这可以用于存储关于集群的任意的、不经常变化的数据而不需要创建索引来存储它。可使用任意以 `cluster.metadata.` 为前缀的键来存储这些数据。例如，可发出以下请求来将集群管理员的电子邮件地址存储在 `cluster.metadata.administrator` 的键中：

[source,js]
-------------------------------
PUT /_cluster/settings
{
  "persistent": {
    "cluster.metadata.administrator": "sysadmin@example.com"
  }
}
-------------------------------
// CONSOLE

IMPORTANT: 用户定义的集群元数据并不是用来存储敏感或机密信息的。任何访问<<cluster-get-settings,集群获取设定>> API 的人都可以查看存储在用户定义的群集元数据中的任何信息，这些信息也将记录在 {es} 日志中。

[[cluster-max-tombstones]]
==== 索引墓碑

集群状态维护索引墓碑，以显式地表示索引已被删除。在集群状态中维护的墓碑的数量是由以下属性控制，该属性不能动态更新：

`cluster.indices.tombstones.size`::

当一个删除发生时，索引墓碑可以防止不属于集群的节点加入集群和删除没发生过似的重新导入该索引。为防止集群状态（数据）变得过于巨大，只会保留最近的 `cluster.indices.tombstones.size` 次删除，默认值是500。增加该值会导致不在集群内的节点错漏超过500个的删除。这应当是罕见的，所以默认值是500。尽管墓碑不会占用很多空间，但若50,000个也可能太大了。

[[cluster-logger]]
==== 日志

可以使用 `logger.` 的前缀来动态更新控制日志的设定。例如，发送以下请求将 `indices.recovery` 模块的日志等级提升到 `DEBUG`：

[source,js]
-------------------------------
PUT /_cluster/settings
{
  "transient": {
    "logger.org.elasticsearch.indices.recovery": "DEBUG"
  }
}
-------------------------------
// CONSOLE


[[persistent-tasks-allocation]]
==== 持续性任务的分配Persistent Tasks Allocations

插件可以生成一种称为持续性任务的任务。这些任务通常是长期运行的，且存储在集群状态中，允许集群全面重启后恢复这些任务。

每次生成一个持续性任务，主节点负责将该任务分配给集群中的一个节点，然后被分配的节点将接到该任务并本地执行。分配持续性任务到节点的过程是由以下属性控制的，该属性能动态更新：

`cluster.persistent_tasks.allocation.enable`::
+
--
激活或者禁用持续性任务的分配：

* `all` -             （默认）允许持续性任务被分配到节点
* `none` -            禁止任何类型的持续性任务的分配

该设定不会影响已经被执行的持续性任务。只有新生成的持续性任务，或必须被重分配的任务（例如一个节点离开集群后），会被该设定影响。
--
